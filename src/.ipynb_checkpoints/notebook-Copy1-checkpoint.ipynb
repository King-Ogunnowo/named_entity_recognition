{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fae9b726-d7d9-4e3e-8166-d84cee711769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from gensim.models import FastText\n",
    "from tensorflow.keras.models import load_model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy\n",
    "\n",
    "# Create spaCy Doc with entities\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "def load_artifacts():\n",
    "    one_hot_encoder = pickle.load(open(\"../models/one_hot_encoder.pkl\", 'rb'))\n",
    "    NER_model = load_model('../models/NER_tensorflow_3_input_model/')\n",
    "    label_encoder = pickle.load(open(\"../models/label_encoder.pkl\", 'rb'))\n",
    "    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    return one_hot_encoder, NER_model, embedding_model, label_encoder\n",
    "\n",
    "one_hot_encoder, NER_model, embedding_model, label_encoder = load_artifacts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a88e28-8d2f-4a7b-b183-8fa72c9254d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def clean_text(text):\n",
    "    text = ' '.join([contractions.fix(word) for word in text.split()])\n",
    "    text = re.sub(\"[^a-zA-Z0-9 ]\", \"\", text)\n",
    "    return text      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc76d49d-c7fa-48a6-a637-930fc58c1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def fetch_pos_tag(text):\n",
    "    tags = np.array([token.tag_ for token in nlp(text)]).reshape(-1, 1)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d334dde6-d114-4c9c-a4d8-5f0ebfcaa43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_vectors(text):\n",
    "    tokens = text.split()\n",
    "    token_embeddings = np.array([embedding_model.encode(token) for token in tokens])\n",
    "    sentence_embeddings = np.array([embedding_model.encode(text) for token in tokens])\n",
    "    return token_embeddings, sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53da67ab-7d0e-4bbc-ab1d-4d76404c2a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I am watching a movie on the TV, can you tell Nathan to please be quiet?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e9fb655-f4bd-4a56-b8f9-fe2358a436ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map BIO tags to character-level spans\n",
    "def bio_to_offsets(text, tags):\n",
    "    entities = []\n",
    "    start, entity_type = None, None\n",
    "\n",
    "    tokens =  text.split()\n",
    "    \n",
    "    for idx, (token, tag) in enumerate(zip(tokens, tags)):\n",
    "        if tag.startswith(\"B-\"):  # Beginning of a new entity\n",
    "            if start is not None:\n",
    "                # Save previous entity\n",
    "                entities.append((start, end, entity_type))\n",
    "            start = len(\" \".join(tokens[:idx])) + (1 if idx > 0 else 0)  # Start char\n",
    "            end = start + len(token)  # End char\n",
    "            entity_type = tag.split(\"-\")[1]  # Extract entity type\n",
    "        elif tag.startswith(\"I-\") and start is not None:  # Inside entity\n",
    "            end = len(\" \".join(tokens[:idx+1]))  # Update end char\n",
    "        else:  # Outside entity\n",
    "            if start is not None:\n",
    "                entities.append((start, end, entity_type))\n",
    "                start, entity_type = None, None\n",
    "\n",
    "    if start is not None:  # Save last entity\n",
    "        entities.append((start, end, entity_type))\n",
    "    return entities\n",
    "\n",
    "# # Convert BIO tags to offsets\n",
    "# entity_offsets = bio_to_offsets(tokens, tags)\n",
    "\n",
    "# # Create spaCy Doc with entities\n",
    "# nlp = spacy.blank(\"en\")\n",
    "# doc = nlp(text)\n",
    "\n",
    "# # Add entities to doc\n",
    "# ents = [Span(doc, doc.char_span(start, end).start, doc.char_span(start, end).end, label=label) \n",
    "#         for start, end, label in entity_offsets if doc.char_span(start, end)]\n",
    "# doc.ents = ents\n",
    "\n",
    "# # Define custom colors for entity types\n",
    "# colors = {\n",
    "#     \"per\": \"#a781f9\",\n",
    "#     \"tim\": \"#e59edb\",\n",
    "#     \"gpe\": \"#faa419\",\n",
    "#     \"geo\": \"#80e5d9\",\n",
    "#     \"org\": \"#4ea8de\",\n",
    "#     \"art\": \"#d3c8a8\",\n",
    "#     \"nat\": \"#81c784\",\n",
    "#     \"eve\": \"#ffb74d\"\n",
    "# }\n",
    "# options = {\"ents\": list(colors.keys()), \"colors\": colors}\n",
    "\n",
    "# # Visualize with displacy\n",
    "# displacy.render(doc, style=\"ent\", options=options, jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab07be6d-9d83-4dca-8846-b0a4fe1b2d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/terra-admin/miniforge3/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/terra-admin/miniforge3/lib/python3.10/site-packages/spacy/displacy/__init__.py:213: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n",
      "  warnings.warn(Warnings.W006)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I am watching a movie on the TV can you please be quiet</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def main(text):\n",
    "    cleaned_text = clean_text(text)\n",
    "    token_embedding, sentence_embedding = generate_vectors(cleaned_text)\n",
    "    pos_tags = one_hot_encoder.transform(fetch_pos_tag(cleaned_text)).toarray()\n",
    "    prediction = label_encoder.inverse_transform(np.argmax(NER_model.predict([token_embedding, pos_tags, sentence_embedding]), axis = 1))\n",
    "    output = {\n",
    "        k:v for k, v in zip(cleaned_text.split(), prediction)\n",
    "    }\n",
    "    entity_offsets = bio_to_offsets(cleaned_text, prediction)\n",
    "    doc = nlp(cleaned_text)\n",
    "    ents = [Span(doc, doc.char_span(start, end).start, doc.char_span(start, end).end, label=label) \n",
    "         for start, end, label in entity_offsets if doc.char_span(start, end)]\n",
    "    doc.ents = ents\n",
    "    colors = {\n",
    "    \"per\": \"#a781f9\",\n",
    "    \"tim\": \"#e59edb\",\n",
    "    \"gpe\": \"#faa419\",\n",
    "    \"geo\": \"#80e5d9\",\n",
    "    \"org\": \"#4ea8de\",\n",
    "    \"art\": \"#d3c8a8\",\n",
    "    \"nat\": \"#81c784\",\n",
    "    \"eve\": \"#ffb74d\",\n",
    "    }\n",
    "    options = {\"ents\": list(colors.keys()), \"colors\": colors}\n",
    "    displacy.render(doc, style=\"ent\", options=options, jupyter=True)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "main(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "048412f1-7c5a-499d-aaea-9179bf9d81fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am watching a movie on the TV, can you please be quiet?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "187bd159-009b-4fb8-ac06-dc12a614d65d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. None expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mone_hot_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos_tags\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/sklearn/utils/_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    319\u001b[0m         )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:1024\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;66;03m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m warn_on_unknown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_unknown \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfrequent_if_exist\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1023\u001b[0m }\n\u001b[0;32m-> 1024\u001b[0m X_int, X_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn_on_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m n_samples, n_features \u001b[38;5;241m=\u001b[39m X_int\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_drop_idx_after_grouping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:194\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[0;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    188\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     ignore_category_indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    193\u001b[0m ):\n\u001b[0;32m--> 194\u001b[0m     X_list, n_samples, n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:45\u001b[0m, in \u001b[0;36m_BaseEncoder._check_X\u001b[0;34m(self, X, force_all_finite)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mPerform custom check_array:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m- convert list of strings to object dtype\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# if not a dataframe, do normal check_array validation\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     X_temp \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(X_temp\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mstr_):\n\u001b[1;32m     47\u001b[0m         X \u001b[38;5;241m=\u001b[39m check_array(X, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/sklearn/utils/validation.py:1058\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1056\u001b[0m     )\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nd \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m   1064\u001b[0m     _assert_all_finite(\n\u001b[1;32m   1065\u001b[0m         array,\n\u001b[1;32m   1066\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[1;32m   1067\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[1;32m   1068\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1069\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with dim 3. None expected <= 2."
     ]
    }
   ],
   "source": [
    "one_hot_encoder.transform([pos_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ea51654-8397-47d9-a464-5d722015b8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(31, 34),\n",
       " (72, 75),\n",
       " (102, 105),\n",
       " (162, 165),\n",
       " (219, 222),\n",
       " (245, 248),\n",
       " (317, 320),\n",
       " (359, 362),\n",
       " (429, 432),\n",
       " (485, 488)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = re.finditer('the', text)\n",
    "[(match.start(), match.end()) for match in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f9062a5-5369-4e24-ab65-09c0f26b712b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Amara and David strolled along the winding gravel path of Oakwood Park, the crisp autumn air carrying the scent of fallen leaves. Amara, her dark braids catching the sunlight, paused to admire a squirrel darting across the grass. 'Reminds me of the parks back home,' she said softly. David, his blond hair tousled by the breeze, nodded with a smile. ‘This is the perfect escape, isn’t it?’ he replied, watching children laugh as they chased after a kite fluttering like a bird against the clear blue sky.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ad63f739-a103-47a2-b8c0-a7432fc31718",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"a b c d e f t t t t\"\n",
    "\n",
    "a = a.replace('t', \"\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f86f2509-8436-4fc4-ae61-bb353a995dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a b c d e f  t t t'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c975e331-c419-42d5-9a57-8e011e66053e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a b c d e f   t t'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a.replace('t', \"\", 1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af97670e-0abd-435e-8332-ac4c345e8cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 'the', Start: 0, End: 2\n",
      "Token: 'boy', Start: 4, End: 6\n",
      "Token: 'jumped', Start: 8, End: 13\n",
      "Token: 'the', Start: 15, End: 17\n",
      "Token: 'fence', Start: 19, End: 23\n"
     ]
    }
   ],
   "source": [
    "text = \"the boy jumped the fence\"\n",
    "\n",
    "# Initialize variables\n",
    "tokens = text.split()\n",
    "positions = []\n",
    "current_position = 0\n",
    "\n",
    "# Calculate start and end positions for each token\n",
    "for token in tokens:\n",
    "    start = text.find(token, current_position)\n",
    "    end = start + len(token) - 1\n",
    "    positions.append((token, start, end))\n",
    "    current_position = end + 1  # Move to the next position after the token\n",
    "\n",
    "# Output the results\n",
    "for token, start, end in positions:\n",
    "    print(f\"Token: '{token}', Start: {start}, End: {end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b359ebbd-b224-42e4-889b-831741efe49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
